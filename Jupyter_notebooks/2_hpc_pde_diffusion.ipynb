{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HPC PDE Solver - The diffusion part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fully understand this notebook, please be sure to have already taken a look at the notebooks:\n",
    "* **0_hpc_mpi_tutorial**\n",
    "* **1_hcp_pde_data**\n",
    "\n",
    "**As before it is adviced to run this Jupyter notebook with 4 processes**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The diffusion part of the mini-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will be dedicated for one function only, namely the diffusion function, this function reproduces exactly the stencil operator (see Figure below). As a quick reminder, remember that, after having discretized the Fisher's equation, **each** grid point, in order to update its new $S$ value, must solve the following equation:\n",
    "\n",
    "$f_{i,j}^{k} := -(4+\\alpha)S_{i,j}^{k}+S_{i-1,j}^{k}+S_{i+1,j}^{k}+S_{i,j-1}^{k}+S_{i,j+1}^{k}+\\beta S_{i,j}^{k}(1-S_{i,j}^{k}) +\\alpha S_{i,j}^{k-1}$\n",
    "\n",
    "And as this process must be repeated for each grid point, we get a set of non-linear equations to solve. The solving process will come in the next notebook. For now, we only want to get the $f_{i,j}^{k}$ matrix of our grid and it's exactly what the diffusion function will do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the idea will be to apply the equation above on each grid point. As we can see, for the current grid point, the equation needs the value of the point on the right, above, on the left and below as well as the value of the previous solution to that point. We can illustrate this process like this (**keep in mind that the same process occurs for every point of the grid**): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](media/picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we initialize the Ipyparallel client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 processes running in parallel\n",
      "IDs of the processes running in parallel [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()\n",
    "print(\"There are \" + str(len(rc)) + \" processes running in parallel\")\n",
    "print(\"IDs of the processes running in parallel\", rc.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "from mpi4py import MPI as mpi\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Hello from process 0\n",
      "[stdout:1] Hello from process 1\n",
      "[stdout:2] Hello from process 2\n",
      "[stdout:3] Hello from process 3\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"Hello from process \" + str(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add our previous code for the **data structures** (you can ignore this cell if you understood the previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "class Discretization: \n",
    "    \n",
    "    def __init__(self, nx, ny, nt, t, d=1.0, r=1000.0, v=0, ic=None):\n",
    "        self.nx = int(nx)                             # x dimension (Number of HORIZONTAL grid points) (int)\n",
    "        self.ny = int(ny)                             # y dimension (Number of VERTICAL grid points) (int)\n",
    "        self.nt = int(nt)                             # number of time steps (int)\n",
    "        self.dx = 1.0 / (nx - 1)                      # distance between each grid points (vertically and horizontally) (double)\n",
    "        self.dt = t / nt                              # time step size (double)\n",
    "        self.d = d                                    # the diffusion coefficient D (double)\n",
    "        self.r = r                                    # the reaction coefficient R (double)\n",
    "        self.alpha = (self.dx**2)/(self.d * self.dt)  # dx^2/(D*dt) (double)\n",
    "        self.beta = self.r * (self.dx**2) / self.d    # R*dx^2/D (double)\n",
    "        \n",
    "        # simulation parameters\n",
    "        self.verbose_output = False      # if you want to print more informations during the simulation\n",
    "        self.custom_init = False         # if the simulation has a customised initial condition\n",
    "        \n",
    "        # verbose output\n",
    "        if v != 0:\n",
    "            self.verbose_output = True \n",
    "        \n",
    "        # we check that the points entered are valid for the customised initial condition\n",
    "        # None is equivalent to False when evaluated in Python\n",
    "        if ic: \n",
    "            for point in ic:\n",
    "                    assert(point[0] >= 0 and point[0] < self.nx)  # checking x coordinate\n",
    "                    assert(point[1] >= 0 and point[1] < self.ny)  # checking y coordinate\n",
    "                    assert(point[2] >= 0.0 and point[2] <= 1.0)   # checking s value\n",
    "            self.points = ic                                        # the custom initial condition is valid\n",
    "            self.custom_init = True\n",
    "       \n",
    "        # statistics variables initialized\n",
    "        self.flops_count = 0   # number of flops\n",
    "        self.iters_cg = 0      # number of Conjugate Gradient iteration\n",
    "        self.iters_newton = 0  # number of Newton iteration\n",
    "\n",
    "class Domain: \n",
    "\n",
    "    def create_dim(self, size):\n",
    "        dividers = []\n",
    "        for i in range(size - 1 , 0, -1):\n",
    "            if (size%i == 0 and i != 1):\n",
    "                dividers.append(i)\n",
    "        if (dividers == []):\n",
    "            return [size, 1]\n",
    "        else: \n",
    "            divider = dividers[len(dividers) // 2]\n",
    "            return [size // divider, divider]\n",
    "    \n",
    "    # constructor for the domain object\n",
    "    def __init__(self, rank, size, discretization, communicator):\n",
    "\n",
    "        # dimensions of the cartesian plan\n",
    "        dims = self.create_dim(size)\n",
    "        \n",
    "        # save the dimension on each axis as an object attribute\n",
    "        self.ndomy = dims[0]\n",
    "        self.ndomx = dims[1]\n",
    "\n",
    "        # generate the cartesian plan given the dimensions (mpi function)\n",
    "        # no periodicity of the dimensions\n",
    "        # no reordering\n",
    "        comm_cart = communicator.Create_cart(dims, [False, False], False)\n",
    "        \n",
    "        # save the cartesian communication group as an object attribute\n",
    "        self.comm_cart = comm_cart\n",
    "\n",
    "        # get the coordinates of the current processor on the newly generated cartesian plan (mpi function)\n",
    "        coords = self.comm_cart.Get_coords(rank)\n",
    "        \n",
    "        # save the coordinates as attributes\n",
    "        self.domy = coords[0] # (int)\n",
    "        self.domx = coords[1] # (int)\n",
    "        \n",
    "        # get the ranks of the domains neighbouring the current domain\n",
    "        # .Shift(direction_on_cartesian_plan, size_of_the_shift)\n",
    "        # row direction is 0 (x-direction)\n",
    "        # column direction is 1 (y-direction)\n",
    "        south_north = self.comm_cart.Shift(0, 1) # tuple (int, int)\n",
    "        west_east = self.comm_cart.Shift(1, 1) # tuple (int, int)\n",
    "    \n",
    "        # save the rank of neighbouring domains as object attributes\n",
    "        self.neighbour_south = south_north[0]\n",
    "        self.neighbour_north = south_north[1]\n",
    "        self.neighbour_west = west_east[0]\n",
    "        self.neighbour_east = west_east[1]\n",
    "\n",
    "        # number of horizontal and vertical grid points of the current domain\n",
    "        self.nx = discretization.nx // self.ndomx # (int)\n",
    "        self.ny = discretization.ny // self.ndomy # (int)\n",
    "\n",
    "        # the starting coordinates of the current (sub)domain in the full grid\n",
    "        # the coordinates start from 0\n",
    "        self.startx = (self.domx) * self.nx # (int)\n",
    "        self.starty = (self.domy) * self.ny # (int)\n",
    "\n",
    "        # we adjust for grid dimensions that, potentially, do not divide evenly between the domains\n",
    "        # we stretch the last element of each dimension to fit the remaining part\n",
    "        # domx and domy start from 0 so we have to do (ndomx - 1) & (ndomy - 1)\n",
    "        if self.domx == (self.ndomx - 1):\n",
    "            self.nx = discretization.nx - self.startx\n",
    "        if self.domy == (self.ndomy - 1):\n",
    "            self.ny = discretization.ny - self.starty\n",
    "        \n",
    "        # the ending coordinates in the full grid of the current domain\n",
    "        self.endx = self.startx + self.nx - 1 # (int)\n",
    "        self.endy = self.starty + self.ny - 1 # (int)\n",
    "\n",
    "        # the total number of grid points of the current domain\n",
    "        self.n_total = self.nx * self.ny # (int)\n",
    "\n",
    "        # mpi values for the current domain saved as object attributes\n",
    "        self.rank = rank\n",
    "        self.size = size\n",
    "        \n",
    "        # fields holding the solutions (Numpy matrices)\n",
    "        self.x_new = np.zeros((self.ny, self.nx), dtype=np.float64) # solution at timestep k (2d)\n",
    "        self.x_old = np.zeros((self.ny, self.nx), dtype=np.float64) # solution at timestep k-1 (2d)\n",
    "       \n",
    "        # fields holding the boundary points when they are received from the neighbouring domains\n",
    "        self.bndN = np.zeros((1, self.nx), dtype=np.float64) # boundary north (1d)\n",
    "        self.bndS = np.zeros((1, self.nx), dtype=np.float64) # boundary east (1d)\n",
    "        self.bndE = np.zeros((1, self.ny), dtype=np.float64) # boundary south (1d)\n",
    "        self.bndW = np.zeros((1, self.ny), dtype=np.float64) # boundary east (1d)\n",
    "\n",
    "        # buffers used during boundaries communication\n",
    "        # to send the boundary points of the current domain\n",
    "        # to its neighbours\n",
    "        self.buffN = np.zeros((1, self.nx), dtype=np.float64) # (1d)\n",
    "        self.buffS = np.zeros((1, self.nx), dtype=np.float64) # (1d)\n",
    "        self.buffE = np.zeros((1, self.ny), dtype=np.float64) # (1d)\n",
    "        self.buffW = np.zeros((1, self.ny), dtype=np.float64) # (1d)\n",
    "     \n",
    "    # function for printing the parameters of the cartesian division among each process\n",
    "    def print(self):\n",
    "        # print, for each process, the domain object\n",
    "        for i in range(0, self.size):\n",
    "            if i == self.rank:  \n",
    "                print(\"Rank \" + str(self.rank) + \"/\" + str(self.size-1))\n",
    "                print(\"At index (\" + str(self.domy) + \",\" + str(self.domx) + \")\")\n",
    "                print(\"Neigh N:S \" + str(self.neighbour_north) + \":\" + str(self.neighbour_south))\n",
    "                print(\"Neigh E:W \" + str(self.neighbour_east) + \":\" + str(self.neighbour_west))\n",
    "                print(\"Coordinates startx:endx  \" + str(self.startx) + \":\" + str(self.endx))\n",
    "                print(\"Coordinates starty:endy  \" + str(self.starty) + \":\" + str(self.endy))\n",
    "                print(\"Local dims \" + str(self.nx) + \" x \" + str(self.ny))\n",
    "                print(\"\")\n",
    "            \n",
    "            # keep the printing ordered and cleaned\n",
    "            MPI.COMM_WORLD.Barrier()\n",
    "            # wait a bit when printing to avoid polluating other printed messages.\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the computations in the diffusion function are made in matricial form using Numpy, not only this form is highly efficient, but it also makes the equations simpler to read by avoiding multiple loops. The only thing to keep in mind when doing this calculation with Numpy is the fact that we use a **slice operators**. A slice operator is simply a range of value, let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "We want everything but the last column of a:\n",
      " [[ 1  2  3  4]\n",
      " [ 6  7  8  9]\n",
      " [11 12 13 14]\n",
      " [16 17 18 19]\n",
      " [21 22 23 24]]\n",
      "Interior grid points of a:\n",
      " [[ 7  8  9]\n",
      " [12 13 14]\n",
      " [17 18 19]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "a = np.array([[1, 2, 3, 4, 5], \n",
    "              [6, 7, 8, 9, 10], \n",
    "              [11, 12, 13, 14, 15], \n",
    "              [16, 17, 18, 19, 20], \n",
    "              [21, 22, 23, 24, 25]]) # a 5x5 Numpy matrix\n",
    "\n",
    "if rank==0:\n",
    "    # we want to print everything of matrix a but not the last column\n",
    "    # the slice on the left of the coma is for rows\n",
    "    # the slice on the right of the coma is for columns, here, column 0 is included but 4 is not included\n",
    "    print(\"We want everything but the last column of a:\\n\", a[:, 0:4])\n",
    "    # Another example for the interior grid points of a (assuming a is a grid)\n",
    "    print(\"Interior grid points of a:\\n\", a[1:4, 1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the diffusion function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing the diffusion function, we will heavily use MPI because, as you have probably already understood, we need to send the boundary points between the domains as illustrated in the figure above where the domain 0, to compute its East boundary, needs the West boundary of the domain 1.\n",
    "\n",
    "The logic of our function will be as follows: \n",
    "* We will use **a non-blocking communication** to send the boundaries of the current domain and receive those of its neighbours.\n",
    "* While the current process wait for those non-blocking communications to succeed, **the process will calculate the points of its interior grid. Indeed, those points do not need the boundaries to be calculated** (see the example above for the interior grid points of domain 2 in a grey square). We play on the fact that the communications are non-blocking and that they do not stop the code execution to make the process more efficient.\n",
    "* Once the interior grid points are calculated **and** the non-blocking communications achieved, our current process has the boudaries of its neighbouring domains, so it can now calculate all its boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first step of our function which is also the hardest one is the non-blocking communication of the boundaries of each domain with their neighbours. Let's illustrate a first prototype of diffusion function called `diffusion_test` which only implements a simple boudary communication between North and South neighbours. East and West neighbours are ignored. **In other terms, each domain will have to receive the boundaries from its potential North and South neighbours (if they exist) and send them its own boundaries.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real `diffusion` function and our `diffusion_test` takes the following parameters: \n",
    "* U is the current solution `x_new`.\n",
    "* S is the resulting $f_{i,j}^{k}$ for the full grid after having applied the stencil operator to all points.\n",
    "* discretization is the discretization object. \n",
    "* domain is the domain object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# a very simple prototype of diffusion function implementing only the North-South non-blocking communication to show the functioning\n",
    "def diffusion_test(U, S, discretization, domain):\n",
    "        \n",
    "    # we create shortcuts for variables heavily used \n",
    "    nx = domain.nx\n",
    "    ny = domain.ny\n",
    "    comm_cart = domain.comm_cart\n",
    "\n",
    "    # we initialize the lists for saving the mpi requests and statuses\n",
    "    # of the non-blocking functions in order to wait for them later\n",
    "    statuses = [MPI.Status()] * 2\n",
    "    requests = [MPI.Request()] * 2\n",
    "    num_requests = 0 # current requests counter\n",
    "    # we initialize the lists for saving the mpi requests and statuses\n",
    "    # of the non-blocking functions in order to wait for them later\n",
    "    statuses = [MPI.Status()] * 2\n",
    "    requests = [MPI.Request()] * 2\n",
    "    num_requests = 0 # current requests counter\n",
    "    \n",
    "    # send the North boundary to the North neighbour if the current domain has a North neighbour\n",
    "    if domain.neighbour_north >= 0:\n",
    "        # wait for the south boundary of the Norht neighbour\n",
    "        # set tag to be the sender's rank\n",
    "        # .Irecv([np_array_received, type_of_members], rank_of_source, message_tag)\n",
    "        # the request is returned and saved to wait for it after\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndN, MPI.DOUBLE], domain.neighbour_north, domain.neighbour_north)\n",
    "        num_requests += 1\n",
    "        \n",
    "        # print immediatly the North boundary (which has not been received yet)\n",
    "        print(\"Domain \" + str(domain.rank) + \" will receive from its North neighbour \" + str(domain.neighbour_north) + \":\\n\", domain.bndN)\n",
    "\n",
    "        # pack North buffer of the current domain to send it\n",
    "        # the current domain makes use of the buffer object to send the array\n",
    "        domain.buffN[0, :] = U[ny-1, :]\n",
    "        \n",
    "        # print the boundary which will be sent to North neighbour\n",
    "        print(\"Domain \" + str(domain.rank) + \" will send to its North neighbour \" + str(domain.neighbour_north) + \":\\n\", domain.buffN)\n",
    "        \n",
    "        # the current domain sends its North boundary which will be received as the South boundary by its North neighbour\n",
    "        # .Isend([np_array_sent, type_of_members], rank_of_destination, message_tag)\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffN, MPI.DOUBLE], domain.neighbour_north, domain.rank)\n",
    "        num_requests += 1\n",
    "        \n",
    "    # same logic for the South boundary, send it to the South neighbour if the current domain has a South neighbour\n",
    "    if domain.neighbour_south >= 0:\n",
    "        \n",
    "        # receive the North boundary of the South neighbour\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndS, MPI.DOUBLE], domain.neighbour_south, domain.neighbour_south)\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # !! we wait for the boundary of South neighbour !!\n",
    "        # be careful to not put this wait function also after the .Irecv of the North domain otherwise you will \n",
    "        # enter a deadlock between processes\n",
    "        # here we wait on purpose to see the result of the exchange\n",
    "        requests[num_requests].Wait()\n",
    "        # ----------------------------------------------------\n",
    "        \n",
    "        num_requests += 1\n",
    "        \n",
    "        print(\"Domain \" + str(domain.rank) + \" will receive from its South neighbour \" + str(domain.neighbour_south) + \":\\n\", domain.bndS)\n",
    "        \n",
    "        # pack the South boundary into the South buffer to send it to the South neighbour\n",
    "        domain.buffS[0, :] = U[0, :]\n",
    "        \n",
    "        print(\"Domain \" + str(domain.rank) + \" will send to its South neighbour \" + str(domain.neighbour_south) + \":\\n\", domain.buffS)\n",
    "        \n",
    "        # send the South boundary to the South neighbour\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffS, MPI.DOUBLE], domain.neighbour_south, domain.rank)\n",
    "        num_requests += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus now initialize the domain 0 matrix `x_new` with some values for its South boundary in order to see the results. Indeed, domain 2 should receive the South boundary of domain 0 as its North boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "x_new for domain 0:\n",
      " [[0.  0. ]\n",
      " [0.1 0.2]] \n",
      "\n",
      "Domain 0 will receive from its North neighbour 2:\n",
      " [[0. 0.]]\n",
      "Domain 0 will send to its North neighbour 2:\n",
      " [[0.1 0.2]]\n",
      "[stdout:1] \n",
      "Domain 1 will receive from its North neighbour 3:\n",
      " [[0. 0.]]\n",
      "Domain 1 will send to its North neighbour 3:\n",
      " [[0. 0.]]\n",
      "[stdout:2] \n",
      "Domain 2 will receive from its South neighbour 0:\n",
      " [[0.1 0.2]]\n",
      "Domain 2 will send to its South neighbour 0:\n",
      " [[0. 0.]]\n",
      "[stdout:3] \n",
      "Domain 3 will receive from its South neighbour 1:\n",
      " [[0. 0.]]\n",
      "Domain 3 will send to its South neighbour 1:\n",
      " [[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "# we initialize the objects\n",
    "discretization = Discretization(4, 4, 100, 0.01)\n",
    "domain = Domain(rank, 4, discretization, comm)\n",
    "\n",
    "# let's put some values in the South boudary of domain 0\n",
    "if rank == 0:\n",
    "    domain.x_new[1, 0] = 0.1\n",
    "    domain.x_new[1, 1] = 0.2 \n",
    "    print(\"x_new for domain 0:\\n\", domain.x_new, \"\\n\")\n",
    "\n",
    "S = np.zeros((domain.ny, domain.nx),  dtype=np.float64)\n",
    "# we use the diffusion_test function to see the exchange North-South\n",
    "diffusion_test(domain.x_new, S, discretization, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the exchange between domain 0 and its North neighbour, domain 2, works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same test but,  **this time with the `.Waitall()` function**. We will remove the `.Wait()` in South communication and just put the `.Waitall()` at the end of all the communications to see what is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# a very simple prototype of diffusion function implementing only the North-South non-blocking communication to show the functioning\n",
    "def diffusion_test(U, S, discretization, domain):\n",
    "        \n",
    "    # we create shortcuts for variables heavily used \n",
    "    nx = domain.nx\n",
    "    ny = domain.ny\n",
    "    comm_cart = domain.comm_cart\n",
    "\n",
    "    # we initialize the lists for saving the mpi requests and statuses\n",
    "    # of the non-blocking functions in order to wait for them later\n",
    "    statuses = [MPI.Status()] * 2\n",
    "    requests = [MPI.Request()] * 2\n",
    "    num_requests = 0 # current requests counter\n",
    "    \n",
    "    # send the North boundary to the North neighbour if the current domain has a North neighbour\n",
    "    if domain.neighbour_north >= 0:\n",
    "        # wait for the south boundary of the Norht neighbour\n",
    "        # set tag to be the sender's rank\n",
    "        # .Irecv([np_array_received, type_of_members], rank_of_source, message_tag)\n",
    "        # the request is returned and saved to wait for it after\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndN, MPI.DOUBLE], domain.neighbour_north, domain.neighbour_north)\n",
    "        num_requests += 1\n",
    "        \n",
    "        # print immediatly the North boundary (which has not been received yet)\n",
    "        print(\"Domain \" + str(domain.rank) + \" will receive from its North neighbour \" + str(domain.neighbour_north) + \":\\n\", domain.bndN)\n",
    "\n",
    "        # pack North buffer of the current domain to send it\n",
    "        # the current domain makes use of the buffer object to send the array\n",
    "        domain.buffN[0, :] = U[ny-1, :]\n",
    "        \n",
    "        # print the boundary which will be sent to North neighbour\n",
    "        print(\"Domain \" + str(domain.rank) + \" will send to its North neighbour \" + str(domain.neighbour_north) + \":\\n\", domain.buffN)\n",
    "        \n",
    "        # the current domain sends its North boundary which will be received as the South boundary by its North neighbour\n",
    "        # .Isend([np_array_sent, type_of_members], rank_of_destination, message_tag)\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffN, MPI.DOUBLE], domain.neighbour_north, domain.rank)\n",
    "        num_requests += 1\n",
    "        \n",
    "    # same logic for the South boundary, send it to the South neighbour if the current domain has a South neighbour\n",
    "    if domain.neighbour_south >= 0:\n",
    "        \n",
    "        # receive the North boundary of the South neighbour\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndS, MPI.DOUBLE], domain.neighbour_south, domain.neighbour_south)\n",
    "        num_requests += 1\n",
    "        \n",
    "        print(\"Domain \" + str(domain.rank) + \" will receive from its South neighbour \" + str(domain.neighbour_south) + \":\\n\", domain.bndS)\n",
    "        \n",
    "        # pack the South boundary into the South buffer to send it to the South neighbour\n",
    "        domain.buffS[0, :] = U[0, :]\n",
    "        \n",
    "        print(\"Domain \" + str(domain.rank) + \" will send to its South neighbour \" + str(domain.neighbour_south) + \":\\n\", domain.buffS)\n",
    "        \n",
    "        # send the South boundary to the South neighbour\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffS, MPI.DOUBLE], domain.neighbour_south, domain.rank)\n",
    "        num_requests += 1\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # !! waiting for all non-blocking communications made to succeed\n",
    "    MPI.Request.Waitall(requests, statuses)\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    print(\"---------------------------------------------------------------------\")                                             \n",
    "    print(\"After the Waitall function\")\n",
    "    print(\"Domain \" + str(domain.rank) + \" has South boundary:\\n\", domain.bndS)\n",
    "    print(\"Domain \" + str(domain.rank) + \" has North boundary:\\n\", domain.bndN) \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same objects as before. Because we removed the `.Wait()`, you should see that the boundary sent by domain 0 is now $[0, 0]$ meaning that we print before getting the actual boundary (**as it is a non-blocking communication**). But now, after the `.Waitall` function, we get the domains with all the boundaries received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "x_new for domain 0:\n",
      " [[0.  0. ]\n",
      " [0.1 0.2]] \n",
      "\n",
      "Domain 0 will receive from its North neighbour 2:\n",
      " [[0. 0.]]\n",
      "Domain 0 will send to its North neighbour 2:\n",
      " [[0.1 0.2]]\n",
      "---------------------------------------------------------------------\n",
      "After the Waitall function\n",
      "Domain 0 has South boundary:\n",
      " [[0. 0.]]\n",
      "Domain 0 has North boundary:\n",
      " [[0. 0.]]\n",
      "\n",
      "[stdout:1] \n",
      "Domain 1 will receive from its North neighbour 3:\n",
      " [[0. 0.]]\n",
      "Domain 1 will send to its North neighbour 3:\n",
      " [[0. 0.]]\n",
      "---------------------------------------------------------------------\n",
      "After the Waitall function\n",
      "Domain 1 has South boundary:\n",
      " [[0. 0.]]\n",
      "Domain 1 has North boundary:\n",
      " [[0. 0.]]\n",
      "\n",
      "[stdout:2] \n",
      "Domain 2 will receive from its South neighbour 0:\n",
      " [[0. 0.]]\n",
      "Domain 2 will send to its South neighbour 0:\n",
      " [[0. 0.]]\n",
      "---------------------------------------------------------------------\n",
      "After the Waitall function\n",
      "Domain 2 has South boundary:\n",
      " [[0.1 0.2]]\n",
      "Domain 2 has North boundary:\n",
      " [[0. 0.]]\n",
      "\n",
      "[stdout:3] \n",
      "Domain 3 will receive from its South neighbour 1:\n",
      " [[0. 0.]]\n",
      "Domain 3 will send to its South neighbour 1:\n",
      " [[0. 0.]]\n",
      "---------------------------------------------------------------------\n",
      "After the Waitall function\n",
      "Domain 3 has South boundary:\n",
      " [[0. 0.]]\n",
      "Domain 3 has North boundary:\n",
      " [[0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "discretization = Discretization(4, 4, 100, 0.01)\n",
    "domain = Domain(rank, 4, discretization, comm)\n",
    "\n",
    "if rank == 0:\n",
    "    domain.x_new[1, 0] = 0.1\n",
    "    domain.x_new[1, 1] = 0.2 \n",
    "    print(\"x_new for domain 0:\\n\", domain.x_new, \"\\n\")\n",
    "\n",
    "S = np.zeros((domain.ny, domain.nx),  dtype=np.float64)\n",
    "diffusion_test(domain.x_new, S, discretization, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, only look at the exchange between domain 0 and 2. Now that non-blocking communication is clear. We can do the real diffusion function by adding: \n",
    "* The West-East communication. \n",
    "* The interior grid points computations after the non-blocking communications but before the `.Waitall` function. \n",
    "* The boundaries points computations after the `.Waitall` function.\n",
    "\n",
    "**This complete diffusion function has been exhaustively commented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# the complete diffusion function\n",
    "def diffusion(U, S, discretization, domain):\n",
    "        \n",
    "    # we create shortcuts for variables heavily used in the function\n",
    "    alpha = discretization.alpha\n",
    "    beta = discretization.beta\n",
    "    nx = domain.nx\n",
    "    ny = domain.ny\n",
    "    comm_cart = domain.comm_cart\n",
    "\n",
    "    # we initialize the lists for saving the mpi requests and statuses of the non-blocking functions in order to wait for them later\n",
    "    # there should be a maximum of 8 requests if the domain is completely surrounded by neighbours\n",
    "    statuses = [MPI.Status()] * 8\n",
    "    requests = [MPI.Request()] * 8\n",
    "    num_requests = 0 # current requests counter\n",
    "\n",
    "    # !! The non-blocking communication !!\n",
    "\n",
    "    # send the North boundary to the North neighbour if the current domain has a North neighbour\n",
    "    if domain.neighbour_north >= 0:\n",
    "        # wait for the south boundary of the Norht neighbour\n",
    "        # set tag to be the sender's rank\n",
    "        # .Irecv([np_array_received, type_of_members], rank_of_source, message_tag)\n",
    "        # the request is returned and saved to wait for it after\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndN, MPI.DOUBLE], domain.neighbour_north, domain.neighbour_north)\n",
    "        num_requests += 1\n",
    "\n",
    "        # pack North buffer of the current domain to send it\n",
    "        # the current domain makes use of the buffer object to send the array\n",
    "        domain.buffN[0, :] = U[ny-1, :]\n",
    "        \n",
    "        # the current domain sends its North boundary which will be received as the South boundary by its North neighbour\n",
    "        # .Isend([np_array_sent, type_of_members], rank_of_destination, message_tag)\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffN, MPI.DOUBLE], domain.neighbour_north, domain.rank)\n",
    "        num_requests += 1\n",
    "    \n",
    "    # same logic for South boundary if there is a South neighbour\n",
    "    if domain.neighbour_south >= 0:\n",
    "\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndS, MPI.DOUBLE], domain.neighbour_south, domain.neighbour_south)\n",
    "        num_requests += 1\n",
    "\n",
    "        domain.buffS[0, :] = U[0, :]\n",
    "\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffS, MPI.DOUBLE], domain.neighbour_south, domain.rank)  \n",
    "        num_requests += 1\n",
    "\n",
    "    # same logic for East boundary if there is a East neighbour\n",
    "    if domain.neighbour_east >= 0:\n",
    "\n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndE, MPI.DOUBLE], domain.neighbour_east, domain.neighbour_east)\n",
    "        num_requests += 1\n",
    "        \n",
    "        domain.buffE[0, :] = U[:, nx-1]\n",
    "    \n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffE, MPI.DOUBLE], domain.neighbour_east, domain.rank)  \n",
    "        num_requests += 1\n",
    "\n",
    "    # same logic for West boundary if there is a West neighbour\n",
    "    if domain.neighbour_west >= 0:\n",
    "        \n",
    "        requests[num_requests] = comm_cart.Irecv([domain.bndW, MPI.DOUBLE], domain.neighbour_west, domain.neighbour_west)\n",
    "        num_requests += 1\n",
    "        \n",
    "        domain.buffW[0, :] = U[:, 0]\n",
    "\n",
    "        requests[num_requests] = comm_cart.Isend([domain.buffW, MPI.DOUBLE], domain.neighbour_west, domain.rank) \n",
    "        num_requests += 1\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # while waiting for the non-blocking communications to proceed\n",
    "    # we can calculate the interior grid points\n",
    "    # it makes the process more efficient by reducing the overhead\n",
    "    # of waiting for the communications to proceed (if it was blocking communications)\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # we initialize the value for the slice operator to avoid repeating calculation\n",
    "    srow = domain.ny - 1\n",
    "    scol = domain.nx - 1\n",
    "    \n",
    "    # interior grid points (refer to the formula of the function f_{i, j}^{k})\n",
    "    S[1:srow, 1:scol] = ( -(4.0 + alpha) * U[1:srow, 1:scol] \n",
    "                        + U[1-1:srow-1, 1:scol] + U[1+1:srow+1, 1:scol] \n",
    "                        + U[1:srow, 1-1:scol-1] + U[1:srow, 1+1:scol+1]\n",
    "                        + beta * U[1:srow, 1:scol] * (1.0 - U[1:srow, 1:scol]) \n",
    "                        + alpha * domain.x_old[1:srow, 1:scol] )\n",
    "\n",
    "    # waiting for all the non-blocking communications to succeed before calculating the boundaries\n",
    "    # .Waitall takes a list of requests to wait for and a list of statuses to update when the requests succeed\n",
    "    MPI.Request.Waitall(requests, statuses)\n",
    "\n",
    "    # East boundary\n",
    "    # ---------------\n",
    "    \n",
    "    # slice operator initialized\n",
    "    srow = domain.ny - 1\n",
    "    scol = domain.nx - 1\n",
    "    \n",
    "    # East boudary calculation\n",
    "    S[1:srow, scol] = ( -(4.0 + alpha) * U[1:srow, scol]\n",
    "                        + U[1-1:srow-1, scol] + U[1+1:srow+1, scol] \n",
    "                        + U[1:srow, scol-1] + domain.bndE[0, 1:srow]\n",
    "                        + beta * U[1:srow, scol] * (1.0 - U[1:srow, scol])\n",
    "                        + alpha * domain.x_old[1:srow, scol]  )\n",
    "    \n",
    "    # West boundary\n",
    "    # ---------------\n",
    "    \n",
    "    # slice operator initialized\n",
    "    srow = domain.ny - 1\n",
    "    scol = 0\n",
    "    \n",
    "    # West boundary calculation\n",
    "    S[1:srow, scol] = ( -(4.0 + alpha) * U[1:srow, scol]\n",
    "                        + U[1:srow, scol+1] + U[1-1:srow-1, scol] + U[1+1:srow+1, scol]\n",
    "                        + alpha * domain.x_old[1:srow, scol] + domain.bndW[0, 1:srow]\n",
    "                        + beta * U[1:srow, scol] * (1.0 - U[1:srow, scol]) )\n",
    "    \n",
    "    # North boundary\n",
    "    # ---------------\n",
    "    \n",
    "    # slice operator\n",
    "    srow = domain.ny - 1\n",
    "    \n",
    "    # North-West corner (which needs the value of the two neighbouring domains on the North and on the West)\n",
    "    scol = 0\n",
    "    S[srow, scol] = ( -(4.0 + alpha) * U[srow, scol]\n",
    "                        + U[srow-1, scol] + domain.bndN[0][scol] \n",
    "                        + domain.bndW[0, srow] + U[srow, scol+1]\n",
    "                        + beta * U[srow, scol] * (1.0 - U[srow, scol])\n",
    "                        + alpha * domain.x_old[srow, scol])\n",
    "    \n",
    "    # North boundary\n",
    "    scol = domain.nx - 1\n",
    "    S[srow, 1:scol] = ( -(4.0 + alpha) * U[srow, 1:scol]\n",
    "                        + U[srow, 1-1:scol-1] + U[srow, 1+1:scol+1] + U[srow-1, 1:scol]\n",
    "                        + alpha * domain.x_old[srow, 1:scol] + domain.bndN[0, 1:scol]\n",
    "                        + beta * U[srow, 1:scol] * (1.0 - U[srow, 1:scol]) )\n",
    "    \n",
    "    # North-East corner\n",
    "    scol = domain.nx - 1\n",
    "    S[srow, scol] = ( -(4.0 + alpha) * U[srow, scol]\n",
    "                        + U[srow, scol-1] + U[srow-1, scol]\n",
    "                        + alpha * domain.x_old[srow, scol] + domain.bndE[0, srow] + domain.bndN[0, scol]\n",
    "                        + beta * U[srow, scol] * (1.0 - U[srow, scol]) )\n",
    "\n",
    "    # South boundary\n",
    "    # ---------------\n",
    "    \n",
    "    # slice operator\n",
    "    srow = 0\n",
    "    \n",
    "    # South-West corner\n",
    "    scol = 0\n",
    "    S[srow, scol] = ( -(4.0 + alpha) * U[srow, scol]\n",
    "                        + U[srow, scol+1] + U[srow+1, scol]\n",
    "                        + alpha * domain.x_old[srow, scol] + domain.bndW[0, srow] + domain.bndS[0, scol]\n",
    "                        + beta * U[srow, scol] * (1.0 - U[srow, scol]) )\n",
    "    \n",
    "    # South boundary\n",
    "    scol = domain.nx - 1\n",
    "    S[srow, 1:scol] = ( -(4.0 + alpha) * U[srow, 1:scol]\n",
    "                        + U[srow, 1-1:scol-1] + U[srow, 1+1:scol+1] + U[srow+1, 1:scol]\n",
    "                        + alpha * domain.x_old[srow, 1:scol] + domain.bndS[0, 1:scol]\n",
    "                        + beta * U[srow, 1:scol] * (1.0 - U[srow, 1:scol]) )\n",
    "\n",
    "    # South-East corner\n",
    "    scol = domain.nx - 1\n",
    "    S[srow, scol] = ( -(4.0 + alpha) * U[srow, scol]\n",
    "                        + U[srow, scol-1] + U[srow+1, scol]\n",
    "                        + alpha * domain.x_old[srow, scol] + domain.bndE[0, srow] + domain.bndS[0, scol]\n",
    "                        + beta * U[srow, scol] * (1.0 - U[srow, scol]) )\n",
    "\n",
    "    # Statistics (for printing at the end)\n",
    "    # flop counts\n",
    "    # 8 flops per point\n",
    "    discretization.flops_count += (\n",
    "            + 12 * (nx - 2) * (ny - 2)  # interior points\n",
    "            + 11 * (nx - 2  +  ny - 2)  # boundaries points\n",
    "            + 11 * 4 )                  # corner points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test it, let's initialize a grid of 4x4, this grid will have one point initialised to 0.2 in the boundary of the domain 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "x_new for domain 0:\n",
      " [[0.  0. ]\n",
      " [0.  0.2]]\n",
      "S matrix for domain 0:\n",
      " [[ 0.00000000e+00  2.00000000e-01]\n",
      " [ 2.00000000e-01 -2.05244444e+02]]\n",
      "[stdout:1] \n",
      "S matrix for domain 1:\n",
      " [[0.  0. ]\n",
      " [0.2 0. ]]\n",
      "[stdout:2] \n",
      "S matrix for domain 2:\n",
      " [[0.  0.2]\n",
      " [0.  0. ]]\n",
      "[stdout:3] \n",
      "S matrix for domain 3:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "\n",
    "discretization = Discretization(4, 4, 100, 0.01)\n",
    "domain = Domain(rank, 4, discretization, comm)\n",
    "\n",
    "if rank == 0:\n",
    "    domain.x_new[1, 1] = 0.2 \n",
    "    print(\"x_new for domain 0:\\n\", domain.x_new)\n",
    "\n",
    "S = np.zeros((domain.ny, domain.nx),  dtype=np.float64)\n",
    "diffusion(domain.x_new, S, discretization, domain)\n",
    "print(\"S matrix for domain \" + str(rank) + \":\\n\", S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4 processes, the previous diffusion went like that: \n",
    "\n",
    "Before the diffusion ($U$ matrix):\n",
    "$$\\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0.2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "After the diffusion ($S$ matrix):\n",
    "$$\\begin{bmatrix} 0 & 0.2 & 0 & 0 \\\\ 0.2 & -205.244444 & 0.2 & 0 \\\\ 0 & 0.2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "We can see that it actually works ! The initial value $0.2$ is diffused !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should be clear by now concerning the diffusion, don't hesitate to modify the code to print more informations if you want to do some tests but, **be careful to the deadlocks** ! As you can see, non-blocking communication is quite difficult to debug and may create situations where it is very uneasy to see who is doing what and if there is a bug in the logic. **That's why non-blocking communication is actually not widely used in practice and programmers prefer blocking communication for its simplicity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
